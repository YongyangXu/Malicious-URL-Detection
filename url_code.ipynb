{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748021f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report\n",
    "import chardet\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "\n",
    "def create_special_chars_set(df):\n",
    "    special_chars = set()\n",
    "    for url in df[\"URL\"]:\n",
    "        special_chars.update(re.findall(r'[-_./:?=&*+()]', url))\n",
    "    special_chars = {re.escape(char) for char in special_chars}  \n",
    "    return special_chars\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, wv, special_chars):\n",
    "        self.wv = wv\n",
    "        self.special_chars = special_chars\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = re.split(f'({\"|\".join(self.special_chars)})+', text)\n",
    "        tokens = [token for token in tokens if token != '' and token is not None] \n",
    "        token_ids = [self.wv.key_to_index[token] for token in tokens if token in self.wv.key_to_index]\n",
    "        return torch.tensor(token_ids)\n",
    "    \n",
    "\n",
    "def preprocess_data():\n",
    "    with open(\"Data/dataset1/train.csv\", \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "\n",
    "    with codecs.open(\"Data/dataset1/train.csv\", \"r\", encoding=result['encoding'], errors='replace') as f:\n",
    "        df_train = pd.read_csv(f)\n",
    "\n",
    "    with open(\"Data/dataset1/valid.csv\", \"rb\") as f:\n",
    "        result = chardet.detect(f.read())\n",
    "\n",
    "    with codecs.open(\"Data/dataset1/valid.csv\", \"r\", encoding=result['encoding'], errors='replace') as f:\n",
    "        df_valid = pd.read_csv(f)\n",
    "\n",
    "    return df_train, df_valid\n",
    "\n",
    "\n",
    "def build_word2vec(df_train):\n",
    "    sentences = [list(url) for url in df_train[\"URL\"]]\n",
    "    wv = Word2Vec(sentences, min_count=1)\n",
    "    return wv\n",
    "\n",
    "\n",
    "class URLDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        url = row[\"URL\"]\n",
    "        label = row[\"label\"]\n",
    "        tokens = self.tokenizer.tokenize(url)\n",
    "        min_len = 6  \n",
    "        if len(tokens) < min_len:\n",
    "            tokens = torch.cat([tokens, torch.zeros(min_len - len(tokens), dtype=torch.long)])\n",
    "        return {\"tokens\": tokens.to(torch.long), \"label\": 1 if label == \"good\" else 0}\n",
    "\n",
    "\n",
    "def load_data(df_train, df_valid, tokenizer):\n",
    "    wv = build_word2vec(df_train)\n",
    "    dataset_train = URLDataset(df_train, tokenizer)\n",
    "    dataset_valid = URLDataset(df_valid, tokenizer)\n",
    "\n",
    "\n",
    "    vocab = wv.wv.key_to_index\n",
    "\n",
    "\n",
    "    url_tensors_train = [item[\"tokens\"] for item in dataset_train]\n",
    "    labels_train = [item[\"label\"] for item in dataset_train]\n",
    "    url_tensors_valid = [item[\"tokens\"] for item in dataset_valid]\n",
    "    labels_valid = [item[\"label\"] for item in dataset_valid]\n",
    "\n",
    "    train_data = [(torch.tensor(x).clone().detach(), torch.tensor(y).clone().detach()) for x, y in\n",
    "                  zip(url_tensors_train, labels_train)]\n",
    "    valid_data = [(torch.tensor(x).clone().detach(), torch.tensor(y).clone().detach()) for x, y in\n",
    "                  zip(url_tensors_valid, labels_valid)]\n",
    "\n",
    "    train_iterator = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    valid_iterator = DataLoader(valid_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    return train_iterator, valid_iterator, vocab\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    urls, labels = zip(*batch)\n",
    "    urls = nn.utils.rnn.pad_sequence(urls, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)  \n",
    "    return urls, labels\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings)\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Sequential(\n",
    "                nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs, pretrained_embeddings.shape[1])),\n",
    "                nn.BatchNorm2d(n_filters),  \n",
    "                nn.ReLU()\n",
    "            ) for fs in filter_sizes]\n",
    "        )\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.embedding(x)\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        tokens, labels = batch\n",
    "        logits = model(tokens.to(device)).squeeze(1)\n",
    "        loss = criterion(logits, labels.to(device).float())\n",
    "        acc = binary_accuracy(logits, labels.to(device))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            tokens, labels = batch\n",
    "            logits = model(tokens.to(device)).squeeze(1)\n",
    "            loss = criterion(logits, labels.to(device).float())\n",
    "            acc = binary_accuracy(logits, labels.to(device))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "\n",
    "\n",
    "epochs = 50  \n",
    "batch_size = 64  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "df_train, df_valid = preprocess_data() \n",
    "wv = build_word2vec(df_train)\n",
    "\n",
    "\n",
    "special_chars = create_special_chars_set(df_train)\n",
    "tokenizer = Tokenizer(wv.wv, special_chars)\n",
    "train_iterator, valid_iterator, vocab = load_data(df_train, df_valid, tokenizer)\n",
    "\n",
    "pretrained_embeddings = torch.FloatTensor(wv.wv.vectors)\n",
    "n_filters = 50  \n",
    "filter_sizes = [2, 3, 4, 5, 6] \n",
    "output_dim = 1\n",
    "dropout = 0.3\n",
    "\n",
    "model = CNN(pretrained_embeddings, n_filters, filter_sizes, output_dim, dropout).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5) \n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_accuracies = []\n",
    "valid_accuracies = []\n",
    "\n",
    "print('start');\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    valid_accuracies.append(valid_acc)\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "\n",
    "    print(\n",
    "        f'Epoch: {epoch + 1}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.3f}, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc:.3f}')\n",
    "\n",
    "print('end');\n",
    "\n",
    "\n",
    "model_save_path = \"url_detection_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "def plot_roc_curve(model, iterator):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_true = []\n",
    "        y_score = []\n",
    "        for batch in iterator:\n",
    "            tokens, labels = batch\n",
    "            logits = model(tokens.to(device)).squeeze(1)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            y_true += labels.cpu().tolist()\n",
    "            y_score += probs.cpu().tolist()\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_roc_curve(model, valid_iterator)\n",
    "\n",
    "\n",
    "def evaluate_final(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        for batch in iterator:\n",
    "            tokens, labels = batch\n",
    "            logits = model(tokens.to(device)).squeeze(1)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = torch.round(probs)\n",
    "            y_true += labels.cpu().tolist()\n",
    "            y_pred += preds.cpu().tolist()\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "evaluate_final(model, valid_iterator, criterion)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(valid_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# loaded_model = CNN(pretrained_embeddings, n_filters, filter_sizes, output_dim, dropout).to(device)\n",
    "# loaded_model.load_state_dict(torch.load(model_save_path))\n",
    "# loaded_model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
